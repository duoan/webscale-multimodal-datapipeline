# CommonCrawl text extraction pipeline

data_loader:
  type: CommonCrawlLoader
  params:
    crawl_id: "CC-MAIN-2024-51"
  num_workers: 32

stages:
  - name: content_filtering
    operators:
      - name: url_filter
        params:
          url_field: "url"
      - name: text_length_filter
        params:
          min_length: 100
          max_length: 100000
          text_field: "text"
      - name: text_exact_deduplicator
        params:
          text_field: "text"
    worker:
      min_replicas: 8
      max_replicas: 8

data_writer:
  type: ParquetDataWriter
  params:
    output_path: "./output/commoncrawl"

executor:
  max_samples: 10000000
  batch_size: 200
  dedup_num_buckets: 4
  rejected_samples:
    enabled: true
  metrics:
    enabled: true
    generate_report: true
    debug_samples_per_operator: 20
