# Example pipeline configuration with metrics and HTML report generation

# Data source
data_loader:
  type: HuggingFaceDataLoader
  params:
    dataset_name: "jp1924/Laion400m-1"
    split: "train"
    streaming: true

# Processing stages
stages:
  # Stage 1: Basic metadata and quality (CPU, Rust-accelerated)
  - name: basic_stage
    operators:
      - name: image_metadata_refiner
      - name: image_technical_quality_refiner  # Rust-accelerated
      - name: image_quality_filter
        params:
          min_width: 128
          min_height: 128
          max_compression_artifacts: 0.8
          min_information_entropy: 0.0
      - name: image_phash_deduplicator
    worker:
      num_replicas: 4
      resources:
        cpu: 1

  # Stage 2: Embedding extraction (GPU)
  - name: embedding_stage
    operators:
      - name: image_clip_embedding_refiner
        params:
          model_name: "ViT-L-14"
          pretrained: "openai"
          device: "auto"
          inference_batch_size: 128
          use_fp16: true
    worker:
      num_replicas: 32
      resources:
        cpu: 2
        gpu: 0.2  # Require GPU for embedding extraction

# Output
data_writer:
  type: ParquetDataWriter
  params:
    output_path: "./parquet_data"
    table_name: "image_profiles"

# Execution settings
executor:
  max_samples: 102400
  batch_size: 1024
  dedup_num_buckets: 4

  # Metrics configuration with HTML report
  metrics:
    enabled: true
    output_path: "./metrics"
    collect_custom_metrics: false
    write_on_completion: true
    generate_report: true  # Generate HTML visualization report

    # Optional: Publish to HuggingFace Space
    # huggingface_repo: "username/metrics-dashboard"  # Your HF Space repo
    # huggingface_token: null  # Uses HF_TOKEN env var if not set
